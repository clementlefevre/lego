{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_path :  /media/ramon/My Passport\n",
      "['/media/ramon/My Passport/Festplatte Startdateien', '/media/ramon/My Passport/$RECYCLE.BIN', '/media/ramon/My Passport/ARTE Palettes - 18 DVD', '/media/ramon/My Passport/Bild_from_Juliane_PC', '/media/ramon/My Passport/catalogChili', '/media/ramon/My Passport/Juliane Acer', '/media/ramon/My Passport/lightroom_catalog', '/media/ramon/My Passport/Pictures', '/media/ramon/My Passport/Pictures all PC Juliane Dec 2017', '/media/ramon/My Passport/Software', '/media/ramon/My Passport/System Volume Information', '/media/ramon/My Passport/tempCatalog']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "def chunk_reader(fobj, chunk_size=1024):\n",
    "    \"\"\"Generator that reads a file in chunks of bytes\"\"\"\n",
    "    while True:\n",
    "        chunk = fobj.read(chunk_size)\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def create_hashes(paths):\n",
    "    hashes = {}\n",
    "    for path in paths:\n",
    "       \n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "           \n",
    "            for filename in filenames:\n",
    "                \n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                mhash = hashlib.md5()\n",
    "                for chunk in chunk_reader(open(full_path, 'rb')):\n",
    "                    mhash.update(chunk)\n",
    "                file_id = (mhash.digest(), os.path.getsize(full_path))\n",
    "                if hashes.get(file_id):\n",
    "                    hashes[file_id].append(full_path)\n",
    "                else:\n",
    "                    hashes[file_id] = [full_path]\n",
    "    return hashes\n",
    "\n",
    "\n",
    "def check_for_duplicates(folder_to_keep, display=False):\n",
    "    folders_to_clean = [folder for folder in os.listdir(\n",
    "        '.') if os.path.isdir(folder)]\n",
    "    folders_to_clean = [\n",
    "        folder for folder in folders_to_clean if folder != folder_to_keep]\n",
    "    print \"folders to clean : \", folders_to_clean\n",
    "\n",
    "    hashes_to_keep = create_hashes([folder_to_keep])\n",
    "    print 'files in ', folder_to_keep, len(hashes_to_keep)\n",
    "\n",
    "    hashes_to_clean = create_hashes(folders_to_clean)\n",
    "    # print hashes_to_clean\n",
    "    print 'files in ', folders_to_clean, len(hashes_to_clean)\n",
    "\n",
    "    for file_hash, files_dup in hashes_to_clean.iteritems():\n",
    "        duplicate = hashes_to_keep.get(file_hash, None)\n",
    "        if duplicate:\n",
    "\n",
    "            if display:\n",
    "                print \"Duplicate found:  %s\" % (files_dup)\n",
    "            map(os.remove, files_dup)\n",
    "\n",
    "    print 'remaining files in folders to clean : {}'.format(len(create_hashes(folders_to_clean)))\n",
    "\n",
    "    for folder in folders_to_clean:\n",
    "        recursive_delete_if_empty(folder)\n",
    "\n",
    "\n",
    "def recursive_delete_if_empty(path):\n",
    "    \"\"\"Recursively delete empty directories; return True\n",
    "    if everything was deleted.\"\"\"\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        # If you also want to delete some files like desktop.ini, check\n",
    "        # for that here, and return True if you delete them.\n",
    "        return False\n",
    "\n",
    "    # Note that the list comprehension here is necessary, a\n",
    "    # generator expression would shortcut and we don't want that!\n",
    "    if all([recursive_delete_if_empty(os.path.join(path, filename))\n",
    "            for filename in os.listdir(path)]):\n",
    "        # Either there was nothing here or it was all deleted\n",
    "        os.rmdir(path)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_dupes_list(root_path):\n",
    "    root_path = root_path or '.'\n",
    "    print \"root_path : \",root_path\n",
    "    folders_to_analyze = next(os.walk(root_path))[1]\n",
    "    folders_to_analyze = [os.path.join(root_path,folder) for folder in folders_to_analyze]\n",
    "    print folders_to_analyze\n",
    "    \n",
    "    hashes = create_hashes(folders_to_analyze)\n",
    "    \n",
    "    pickle.dump(hashes, open(\"hashes.p\", \"wb\"))\n",
    "    \n",
    "    hashes = pickle.load(open(\"hashes.p\", \"rb\"))\n",
    "    df = pd.DataFrame.from_dict(hashes,orient='index')\n",
    "    df_stacked = pd.DataFrame(df.stack(),columns=['path'])\n",
    "    df_stacked = df_stacked.reset_index(level=0)\n",
    "    df_stacked.rename(columns={'level_0':'hash'}, inplace=True)\n",
    "    df_stacked = df_stacked.sort_values('hash')\n",
    "    df_stacked['count'] =df_stacked.groupby('hash')[\"path\"].transform(\"count\")\n",
    "    df_stacked = df_stacked.sort_values('count',ascending=False)\n",
    "    df_files_paths = df_stacked['path'].str.split('/', 4, expand=True)\n",
    "    df_stacked['file_name']=df_stacked.path.str.extract('([^/]+$)')\n",
    "    df_stacked.drop('path',1,inplace=True)\n",
    "    df_final = pd.concat([df_files_paths, df_stacked], axis=1)\n",
    "    df_final = df_final[df_final['count']>1]\n",
    "    df_final=df_final.sort_values('hash')\n",
    "    df_final.to_csv('duplicates_list.csv')\n",
    "    df_summary = df_final.groupby([0,1])[['hash']].count().sort_values('hash',ascending=False)\n",
    "    df_summary.to_csv('duplicates_list_summary.csv')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        folder_to_keep = sys.argv[1]\n",
    "    except:\n",
    "        print 'no arg given !'\n",
    "    \n",
    "    create_dupes_list(\"/media/ramon/My Passport\")\n",
    "\n",
    "    #check_for_duplicates(folder_to_keep, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Festplatte Startdateien',\n",
       " '$RECYCLE.BIN',\n",
       " 'ARTE Palettes - 18 DVD',\n",
       " 'Bild_from_Juliane_PC',\n",
       " 'catalogChili',\n",
       " 'Juliane Acer',\n",
       " 'lightroom_catalog',\n",
       " 'Pictures',\n",
       " 'Pictures all PC Juliane Dec 2017',\n",
       " 'Software',\n",
       " 'System Volume Information',\n",
       " 'tempCatalog']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(os.walk('/media/ramon/My Passport'))[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
